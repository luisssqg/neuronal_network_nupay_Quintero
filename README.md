# Funciones de ActivaciÃ³n y sus Derivadas

Este proyecto implementa y grafica las funciones de activaciÃ³n **Sigmoid, ReLU y TanH**, junto con sus respectivas derivadas. Estas funciones son esenciales en redes neuronales para introducir no linealidad y permitir el aprendizaje de patrones complejos.

## ğŸ“Œ Requisitos

Antes de ejecutar el cÃ³digo, asegÃºrate de tener instaladas las siguientes dependencias:

```bash
pip install numpy matplotlib
ğŸš€ EjecuciÃ³n
Para ejecutar el script y visualizar las grÃ¡ficas, simplemente corre:

bash
Copiar
Editar
python main.py
ğŸ“‚ Estructura del Proyecto
css
Copiar
Editar
project/
â”‚â”€â”€ src/
â”‚   â”‚â”€â”€ sigmoid.py
â”‚   â”‚â”€â”€ relu.py
â”‚   â”‚â”€â”€ tanh.py
â”‚â”€â”€ main.py
â”‚â”€â”€ README.md
ğŸ“Š DescripciÃ³n del CÃ³digo
main.py:

Importa las funciones de activaciÃ³n y sus derivadas desde la carpeta src/.
Genera un conjunto de valores en el rango [-5, 5].
Grafica las funciones Sigmoid, ReLU y TanH junto con sus derivadas.
Muestra todas las grÃ¡ficas en una figura de Matplotlib.
src/sigmoid.py:

Implementa la funciÃ³n sigmoid(x) y su derivada sigmoid_derivative(x).
src/relu.py:

Implementa la funciÃ³n relu(x) y su derivada relu_derivative(x).
src/tanh.py:

Implementa la funciÃ³n tanh(x) y su derivada tanh_derivative(x).
ğŸ“· Ejemplo de Salida
El script generarÃ¡ una figura con seis grÃ¡ficos mostrando cada funciÃ³n y su derivada:

Sigmoid y su derivada
ReLU y su derivada
TanH y su derivada
ğŸ“Œ Notas
AsegÃºrate de que la carpeta src/ contenga correctamente los archivos de las funciones de activaciÃ³n.
Si usas un entorno virtual, activa el entorno antes de ejecutar el cÃ³digo.
